---
title: "Prediction Assignment"
author: "Fahmi Adi Nugraha"
date: "04/27/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')
```

# Overview

The goal of this assignment is to train a machine learning algorithm to identify
the quality of a specific exercise routine, namely, bicep curls using dumbbells.
In the provided data, participants of a study were trained to perform bicep
curls correctly as well as incorrectly, emulating four common mistakes.
Each of these five "types" of bicep curls have been given their own category
labeled "A" through "E". Throughout each session, the motion of each participant
was measured using accelerometers worn on the arms and waist. The trained
machine learning algorithm should be able to correctly distiguish between each
category of bicep curls using the measurements from the accelerometer.

# EDA and Data cleaning

## Feature removal

### Remove features with missing values

We begin the entire process of training a machine learning algorithm by tidying
up the data. We will first load the data and then split it into training and
test sets with a 70/30 ratio.

```{r load and split data, warning = FALSE, message = FALSE}
# Load the necessary libraries
library(tidyverse)
library(magrittr)

# Load the data
pml_data <- read_csv('pml_training.csv')

# Split data into training and test sets
set.seed(1234)

in_train <- createDataPartition(pml_data$classe, p = 0.7, list = F)
pml_train <- pml_data[in_train, ]
pml_test <- pml_data[-in_train, ]
```

A quick pass of the training set through the `glimpse()` function reveals a
large number of features with missing values.

```{r glimpse data}
# Glimpse training data
pml_train %>% glimpse()
```

We can get an exact count of the number of missing values in each feature as
shown below.

```{r missing value counts}
# Get counts of missing values in each feature
pml_missing <- colSums(is.na(pml_train)) %>%
    enframe(name = 'col_name', value = 'num_missing')
pml_missing_counts <- pml_missing %>% count(num_missing)
pml_missing_counts
```

Here we see that there are 100 features with at least
`r pml_missing_counts %>% pluck('num_missing', 2)` missing values. If we
consider that there are a total of `r dim(pml_train)[1]` training examples, it's
clear that imputation or other strategies to "fill in" the missing data won't be
very effective. Thus, we will be dropping all columns with missing values
leaving us with 60 variables from the orignial 160.

```{r remove cols with missing vals, message = F, warning = F}
# Get column names
cols_missing_data <- pml_count_missing %>% filter(num_missing > 0) %>%
    pluck('col_name')

# Remove all cols with missing data
pml_train %<>% select(-all_of(cols_missing_data))
```

### Remove unnecessary features

Out of the remaining features, the following seem fit for removal

- `X1`
- `user_name`
- `raw_timestamp_part_1`
- `raw_timestamp_part_2`
- `cvtd_timestamp`
- `new_window`
- `num_window`

#### `X1`

If we look at this variable, we see that it's simply a copy of the index of the
original data structure that was used to store the data.

```{r view X1 var}
pml_train %>% select(X1) %>% head(10)
```

For this reason, it seems safe to omit this feature.

```{r remove X1 var}
pml_train %<>% select(-X1)
```

#### `user_name` and `\*timestamp\*`

It's my understanding that the ultimate goal of our trainined algorithm is to
be able to predict the quality of the bicep curls irrespective of who performed
them or when they were performed. Thus, it seems reasonable to remove these
features.

```{r remove user_name and timestamp vars}
cols_name_timestamp <- c('user_name', 'raw_timestamp_part_1',
                         'raw_timestamp_part_2', 'cvtd_timestamp')
pml_train %<>% select(-all_of(cols_name_timestamp))
```

#### `new_window` and `num_window`

These "features" are actually the values of the parameters of the sliding window
that the researchers in the original study used to extract features related to
the euler angles of each sensor. Due to this, it seems safe to assume that these
values are not related at all to the quality of the bicep curls and can be
removed.

```{r remove window vars}
cols_window <- c('new_window', 'num_window')
pml_train %<>% select(-all_of(cols_window))
```

## Convert `classe` feature to categorical variable

If we go back to the output of the glimpse function from one of the previous
sub-sections, we see that the `classe` feature is of type `char`. We'll need to
convert this feature to type `fct` in order for our classification algorithms to
work properly. We will also convert the `classe` feature in the test set to a
factor as well in order to obtain model predictions later on.

```{r convert classe to fct}
pml_train %<>% mutate(classe = as.factor(classe))
pml_test %<>% mutate(classe = as.factor(classe))
```

## Identify outliers and other problems with the remaining features

Here we'll quickly print out summary statistics and rough histograms for each of
the remaining features.

```{r feature summary stats, message = FALSE}
# Load the skimr library
library(skimr)

# Print summary
pml_train %>% select(-classe) %>% skim()
```

From the resulting output, we can see that very few of the variables have
distributions approaching normal. We can also see what appears to be an outlier
in the `magnet_dumbbell_y` feature. However, this isn't really a problem for us
as the classifier that we plan to train is fairly robust to outliers. We can go
a bit further and take a look at a summary of the contents of each feature.

```{r xray anomalies, message = FALSE}
# Load necessary library
library(xray)

# Print summary of anomalies
pml_train %>% select(-classe) %>% anomalies()
```

As we saw before, none of the remaining features has missing values as can be
seen from the `qNA` and `qBlank` columns. There do appear to be an excess of
zeros in some of the features but if we look at the names of these features
they all seem to be measuring movement in the forearm and waist. Considering the
exercise that is being measured is bicep curls, it makes sense that there would
be periods where there is no motion detected in the forearms and the waist
(particularly in the y-axis in the case of the waist measurement).

Let us also take a quick look at the number of observations per class.

```{r obs per class}
pml_train %>% count(classe)
```

The classes appear to be fairly balanced although there do seem to be more
observations in the `A` classe compared to the other classes. Overall, no
further transformations on the features are required.

# Feature selection

With our remaining features we will now perform feature selection using
Correlation-based Feature Selection (CFS). For our purposes, we will use the
implementation of CFS provided in the `FSelector` package.

```{r feature selection using cfs, message = FALSE}
# Load FSelector package
library(FSelector)

# Perform feature selection using CFS
best_features <- cfs(classe ~ ., pml_train)

# Display the selected features
best_features
```

The algorithm has selected `r length(best_features)` variables out of a total
of `r dim(pml_train)[2]` features which is a significant reduction. Before
we train our model, let us remove all of the unnecessary features from our
training set.

```{r remove unselected features}
pml_train_best_feats <- pml_train %>% select(all_of(features_cfs), classe)
```

# Model training and evaluation

For this project, we will use an XGBoost tree classifier. The training process
includes hyperparameter tuning using a grid search with 5-fold cross validation
to select the best set of hyperparameters for our model. Additionally, we will
be making use of the `doParallel` library in order to speed up model training.

```{r train xgbTree model, eval = FALSE}
# Load the necessary libraries
library(caret)
library(xgboost)
library(doParallel)

# Set cross validation parameters for caret's train function and create
# hyperparamter grid
tr_ctrl <- trainControl(method = 'cv', number = 5)
xgb_tune <- expand.grid(nrounds = c(50, 100, 150),
                        max_depth = c(1, 3, 5, 7),
                        eta = c(0.01, 0.1, 0.5),
                        gamma = c(0.1, 0.5, 1, 5, 10, 50),
                        colsample_bytree = c(1, 3, 5),
                        min_child_weight = c(1, 3, 5),
                        subsample = c(0.5, 0.6, 0.7, 0.8))

# Train model with parallel processing enabled
set.seed(56765)

cl <- makeForkCluster(detectCores() - 1)
registerDoParallel(cl)
pml_xgb <- train(classe ~ ., data = pml_train_best_feats, method = 'xgbTree',
                 trControl = tr_ctrl, tuneGrid = xgb_tune, nthread = 1)
stopCluster(cl)
```

```{r load saved model, include = FALSE}
# Load trained model
load('trained_model/pml_xgb.Rda')
```

After training our model it is now time to evaluate its performance. Below is
the confusion matrix of our model's predictions on the test set.

```{r plot confusion matrix}
# Save confusion matrix object
conf_mat <- confusionMatrix(predict(pml_xgb, pml_test), pml_test$classe,
                            mode = 'prec_recall')

# Get and prep table from confusion matrix objec
conf_mat_tibble <- conf_mat$table %>% as_tibble() %>%
    rename(prediction = Prediction, actual = Reference, freq = n)

# Plot confusion matrix
conf_mat_tibble %>% ggplot(aes(x = actual, y = prediction, fill = freq)) +
    geom_tile() +
    geom_text(aes(label = freq), size = 3, color = 'gray20') +
    scale_fill_gradient(low = 'white', high = 'steelblue', name = 'Frequency') +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title = 'Confusion Matrix of XGBoost Model Predictions')
```

Here we can see that the model seems to do a good job overall in predicting the
correct classes. This is particularly true of the `E` class, where the majority
of the predictions were correct with few false positives and false negatives.
While the other classes had comparitively more false positives and false
negatives, the total amounts of both types of errors is small relative to the
total number of instances in each class. This is reflected in the per-class
precision, recall, and F1 metrics shown below.

```{r display per-class metrics}
# Display the per-class metrics
conf_mat %>% as.matrix(what = 'classes') %>%
    as.data.frame() %>% .[5:7 , ] %>%
    kable() %>%
    kable_styling(bootstrap_options = 'striped', full_width = F)
```

The values of each metric for each class are all close to 1 indicating that the
model is able to correctly identify a given class over the vast majority of
instances.

```{r display overall metrics}
# Load the necessary libraries
library(knitr)
library(kableExtra)

# Display the overall metrics
conf_mat %>% as.matrix(what = 'overall') %>%
    as.data.frame() %>%
    rename(value = V1) %>%
    kable() %>%
    kable_styling(bootstrap_options = 'striped', full_width = F)
```

The value model's accuracy and kappa metrics also seem to show that the model's
performance is quite good.

# Summary